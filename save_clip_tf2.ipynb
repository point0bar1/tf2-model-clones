{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "KZPUnFQBsArm"
   },
   "outputs": [],
   "source": [
    "TF2_FILE = 'clip-vitb-32-tf2/weights'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13560,
     "status": "ok",
     "timestamp": 1640387963844,
     "user": {
      "displayName": "MITCHELL HILL",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "00701271769183393256"
     },
     "user_tz": 300
    },
    "id": "KjpvBBqSP18a",
    "outputId": "531b9ad3-3783-4991-f40d-cae3cefec77d"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# install libraries\n",
    "#!pip install ftfy regex tqdm\n",
    "# import clip code\n",
    "if not os.path.exists('CLIP/'):\n",
    "    !git clone https://github.com/openai/CLIP.git\n",
    "# add cloned git directories to path (otherwise colab can't find them)\n",
    "sys.path.insert(0, 'CLIP/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11467,
     "status": "ok",
     "timestamp": 1640387975305,
     "user": {
      "displayName": "MITCHELL HILL",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "00701271769183393256"
     },
     "user_tz": 300
    },
    "id": "N2WJsBeEQQ36",
    "outputId": "2ccd2456-5978-48f8-f78a-82e1a47ad864"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label probs: [[0.9927937  0.00421067 0.0029957 ]]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import clip\n",
    "from clip.simple_tokenizer import SimpleTokenizer as _Tokenizer\n",
    "from PIL import Image\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "image = preprocess(Image.open(os.path.join(\"CLIP.png\"))).unsqueeze(0).to(device)\n",
    "text = clip.tokenize([\"a diagram\", \"a dog\", \"a cat\"]).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    image_features = model.encode_image(image)\n",
    "    text_features = model.encode_text(text)\n",
    "    \n",
    "    logits_per_image, logits_per_text = model(image, text)\n",
    "    probs = logits_per_image.softmax(dim=-1).cpu().numpy()\n",
    "\n",
    "print(\"Label probs:\", probs)  # prints: [[0.9927937  0.00421068 0.00299572]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 31,
     "status": "ok",
     "timestamp": 1640387975307,
     "user": {
      "displayName": "MITCHELL HILL",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "00701271769183393256"
     },
     "user_tz": 300
    },
    "id": "KjwT2CG_SCXF",
    "outputId": "63fdd7e1-566a-4b39-e84a-f78da4d99ef7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-3.90530936e-03, -6.32541208e-03,  7.35073257e-03, ...,\n",
       "        -1.06596146e-02, -2.27636080e-02, -1.09076742e-02],\n",
       "       [-2.60810871e-02,  8.79530329e-03, -1.17371846e-02, ...,\n",
       "        -1.20190559e-02, -2.40586717e-02, -2.19290163e-02],\n",
       "       [-1.96484327e-02, -6.67110318e-03, -9.05930717e-03, ...,\n",
       "         4.57819877e-03, -2.06920728e-02, -8.71497300e-03],\n",
       "       ...,\n",
       "       [ 8.50279909e-03,  1.02194364e-03,  2.03663092e-02, ...,\n",
       "         1.48675805e-02,  1.76269542e-02, -1.47524709e-03],\n",
       "       [-1.67414418e-03,  7.30483516e-05, -4.19964641e-03, ...,\n",
       "        -3.40962294e-03, -3.92947206e-03, -5.52894235e-05],\n",
       "       [-6.02601049e-03,  2.02100957e-03,  4.96737135e-04, ...,\n",
       "        -3.34585714e-03, -9.85872559e-03, -2.33900530e-04]], dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.token_embedding.weight.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "SidA8MoXVL6B"
   },
   "outputs": [],
   "source": [
    "# TF2 replica of CLIP\n",
    "\n",
    "from collections import OrderedDict\n",
    "from typing import Tuple, Union\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "\n",
    "class QuickGELU(keras.layers.Layer):\n",
    "    def call(self, x):\n",
    "        return x * keras.activations.sigmoid(1.702 * x)\n",
    "\n",
    "\n",
    "class ResidualAttentionBlock(keras.layers.Layer):\n",
    "    def __init__(self, d_model, n_head, attn_mask=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.attn = keras.layers.MultiHeadAttention(n_head, d_model // n_head)\n",
    "        self.ln_1 = keras.layers.LayerNormalization(epsilon=1e-5)\n",
    "        self.mlp = keras.Sequential([\n",
    "            keras.layers.Dense(d_model * 4),\n",
    "            QuickGELU(),\n",
    "            keras.layers.Dense(d_model)\n",
    "        ])\n",
    "        self.ln_2 = keras.layers.LayerNormalization(epsilon=1e-5)\n",
    "        self.attn_mask = attn_mask\n",
    "\n",
    "    def attention(self, x):\n",
    "        return self.attn(x, x, x, attention_mask=self.attn_mask)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = x + self.attention(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class Transformer(keras.layers.Layer):\n",
    "    def __init__(self, width, layers, heads, attn_mask=None):\n",
    "        super().__init__()\n",
    "        self.width = width\n",
    "        self.layers = layers\n",
    "        self.resblocks = keras.Sequential([ResidualAttentionBlock(width, heads, attn_mask) for _ in range(layers)])\n",
    "\n",
    "    def call(self, x):\n",
    "        return self.resblocks(x)\n",
    "\n",
    "\n",
    "class VisionTransformer(keras.layers.Layer):\n",
    "    def __init__(self, input_resolution, patch_size, width, layers, heads, output_dim):\n",
    "        super().__init__()\n",
    "        self.input_resolution = input_resolution\n",
    "        self.output_dim = output_dim\n",
    "        self.conv1 = keras.layers.Conv2D(width, kernel_size=patch_size, strides=patch_size, use_bias=False)\n",
    "\n",
    "        scale = width ** -0.5\n",
    "        self.class_embedding = self.add_weight(name='cl_emb', shape=(1, 1, width,), trainable=True)\n",
    "        self.positional_embedding = self.add_weight(name='pos_emb', shape=((input_resolution // patch_size) ** 2 + 1, width), trainable=True)\n",
    "        self.ln_pre = keras.layers.LayerNormalization(epsilon=1e-5)\n",
    "\n",
    "        self.transformer = Transformer(width, layers, heads)\n",
    "\n",
    "        self.ln_post = keras.layers.LayerNormalization(epsilon=1e-5)\n",
    "        self.proj = self.add_weight(name='proj', shape=(width, output_dim), trainable=True)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.conv1(x)  # shape = [*, grid, grid, width]\n",
    "        x = tf.reshape(x, (x.shape[0], -1, x.shape[3]))  # shape = [*, grid ** 2, width]\n",
    "        x = tf.concat((tf.tile(self.class_embedding, (x.shape[0], 1, 1)), x), axis=1)\n",
    "        x = x + self.positional_embedding\n",
    "        x = self.ln_pre(x)\n",
    "\n",
    "        x = self.transformer(x)\n",
    "\n",
    "        x = self.ln_post(tf.gather(x, 0, axis=1))\n",
    "\n",
    "        if self.proj is not None:\n",
    "            x = tf.linalg.matmul(x, self.proj)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class CLIPTF2(keras.Model):\n",
    "    def __init__(self,\n",
    "                 embed_dim,\n",
    "                 # vision\n",
    "                 image_resolution,\n",
    "                 vision_layers,\n",
    "                 vision_width,\n",
    "                 vision_patch_size,\n",
    "                 # text\n",
    "                 context_length,\n",
    "                 vocab_size,\n",
    "                 transformer_width,\n",
    "                 transformer_heads,\n",
    "                 transformer_layers\n",
    "                 ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.context_length = context_length\n",
    "        self._tokenizer = _Tokenizer()\n",
    "\n",
    "        vision_heads = vision_width // 64\n",
    "        self.visual = VisionTransformer(\n",
    "            input_resolution=image_resolution,\n",
    "            patch_size=vision_patch_size,\n",
    "            width=vision_width,\n",
    "            layers=vision_layers,\n",
    "            heads=vision_heads,\n",
    "            output_dim=embed_dim\n",
    "        )\n",
    "\n",
    "        self.transformer = Transformer(\n",
    "            width=transformer_width,\n",
    "            layers=transformer_layers,\n",
    "            heads=transformer_heads,\n",
    "            attn_mask=self.build_attention_mask()\n",
    "        )\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.token_embedding = keras.layers.Embedding(vocab_size, transformer_width)\n",
    "        self.positional_embedding = self.add_weight(name='pos_emb', shape=(self.context_length, transformer_width), trainable=True)\n",
    "        self.ln_final = keras.layers.LayerNormalization(epsilon=1e-5)\n",
    "\n",
    "        self.text_projection = self.add_weight(name='text_proj', shape=(transformer_width, embed_dim), trainable=True)\n",
    "        #self.logit_scale = tf.constant(np.log(1 / 0.07), dtype=tf.float32)\n",
    "\n",
    "    def build_attention_mask(self):\n",
    "        mask = tf.ones(shape=(self.context_length, self.context_length))\n",
    "        # lower triangular attention\n",
    "        mask = tf.linalg.band_part(mask, -1, 0)\n",
    "        return mask\n",
    "\n",
    "    def encode_image(self, image):\n",
    "        return self.visual(image)\n",
    "\n",
    "    def encode_text(self, text):\n",
    "        x = self.token_embedding(text)  # [batch_size, n_ctx, d_model]\n",
    "\n",
    "        x = x + self.positional_embedding\n",
    "        x = self.transformer(x)\n",
    "        x = self.ln_final(x)\n",
    "\n",
    "        # x.shape = [batch_size, n_ctx, transformer.width]\n",
    "        # take features from the eot embedding (eot_token is the highest number in each sequence)\n",
    "        x = tf.linalg.matmul(tf.gather(x, tf.math.argmax(text, axis=-1), axis=1, batch_dims=1), self.text_projection)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def tokenize(self, texts, context_length=77, truncate=False):\n",
    "        if isinstance(texts, str):\n",
    "            texts = [texts]\n",
    "\n",
    "        sot_token = self._tokenizer.encoder[\"<|startoftext|>\"]\n",
    "        eot_token = self._tokenizer.encoder[\"<|endoftext|>\"]\n",
    "        all_tokens = [[sot_token] + self._tokenizer.encode(text) + [eot_token] for text in texts]\n",
    "        result = tf.zeros((len(all_tokens), context_length), dtype=tf.int32)\n",
    "\n",
    "        for i, tokens in enumerate(all_tokens):\n",
    "            if len(tokens) > context_length:\n",
    "                if truncate:\n",
    "                    tokens = tokens[:context_length]\n",
    "                    tokens[-1] = eot_token\n",
    "                else:\n",
    "                    raise RuntimeError(f\"Input {texts[i]} is too long for context length {context_length}\")\n",
    "            tokens = tokens + [0 for _ in range(max(0, context_length - len(tokens)))]\n",
    "            result = tf.tensor_scatter_nd_update(result, tf.constant(i, shape=(1, 1)), tf.reshape(tf.constant(tokens, dtype=tf.int32), (1, -1)))\n",
    "\n",
    "        return result\n",
    "\n",
    "    def call(self, image, text, logit_scale=100):\n",
    "        image_features = self.encode_image(image)\n",
    "        text_features = self.encode_text(text)\n",
    "\n",
    "        # normalized features\n",
    "        image_features = image_features / tf.norm(image_features, axis=-1, keepdims=True)\n",
    "        text_features = text_features / tf.norm(text_features, axis=-1, keepdims=True)\n",
    "\n",
    "        # cosine similarity as logits\n",
    "        #logit_scale = tf.math.exp(self.logit_scale)\n",
    "\n",
    "        logits_per_image = logit_scale * tf.linalg.matmul(image_features, tf.transpose(text_features, (1, 0)))\n",
    "        logits_per_text = tf.transpose(logits_per_image, (1, 0))\n",
    "\n",
    "        # shape = [global_batch_size, global_batch_size]\n",
    "        return logits_per_image, logits_per_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8838,
     "status": "ok",
     "timestamp": 1640387988433,
     "user": {
      "displayName": "MITCHELL HILL",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "00701271769183393256"
     },
     "user_tz": 300
    },
    "id": "0O11eXF0s3KR",
    "outputId": "e9c21ab9-12bf-444f-c81a-e805df6470c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[3.304957  3.8573132 4.0554957]], shape=(1, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "clip_tf = CLIPTF2(embed_dim=512, image_resolution=224, vision_layers=12, vision_width=768, vision_patch_size=32,\n",
    "                  context_length=77, vocab_size=49408, transformer_width=512, transformer_layers=12, transformer_heads=8)\n",
    "\n",
    "text_tf = clip_tf.tokenize([\"a diagram\", \"a dog\", \"a cat\"])\n",
    "image_tf = tf.transpose(tf.constant(image.numpy()), (0, 2, 3, 1))\n",
    "\n",
    "logits_per_image_tf, logits_per_text_tf = clip_tf(image_tf, text_tf)\n",
    "print(logits_per_image_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1488,
     "status": "ok",
     "timestamp": 1640387989902,
     "user": {
      "displayName": "MITCHELL HILL",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "00701271769183393256"
     },
     "user_tz": 300
    },
    "id": "MozhyfcK57FT",
    "outputId": "1778713b-83a6-4578-c824-49d1f2e232c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patch Embeddings for ViT.\n",
      "ViT ResBlock 1\n",
      "ViT ResBlock 2\n",
      "ViT ResBlock 3\n",
      "ViT ResBlock 4\n",
      "ViT ResBlock 5\n",
      "ViT ResBlock 6\n",
      "ViT ResBlock 7\n",
      "ViT ResBlock 8\n",
      "ViT ResBlock 9\n",
      "ViT ResBlock 10\n",
      "ViT ResBlock 11\n",
      "ViT ResBlock 12\n",
      "Embeddings for Text Transformer.\n",
      "Text ResBlock 1\n",
      "Text ResBlock 2\n",
      "Text ResBlock 3\n",
      "Text ResBlock 4\n",
      "Text ResBlock 5\n",
      "Text ResBlock 6\n",
      "Text ResBlock 7\n",
      "Text ResBlock 8\n",
      "Text ResBlock 9\n",
      "Text ResBlock 10\n",
      "Text ResBlock 11\n",
      "Text ResBlock 12\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'UnreadVariable' shape=(512, 512) dtype=float32, numpy=\n",
       "array([[-0.01043701,  0.01422882, -0.00836945, ..., -0.00688171,\n",
       "        -0.01246643,  0.00120258],\n",
       "       [ 0.00536346,  0.00133991, -0.00360298, ...,  0.0026207 ,\n",
       "         0.01364899, -0.02009583],\n",
       "       [ 0.00286102,  0.00314713,  0.01821899, ...,  0.00336075,\n",
       "         0.00518799, -0.00634384],\n",
       "       ...,\n",
       "       [ 0.00939178,  0.03060913,  0.0135498 , ...,  0.01597595,\n",
       "         0.00135612, -0.0109787 ],\n",
       "       [-0.01131439,  0.00466537,  0.0016737 , ..., -0.00434875,\n",
       "        -0.01869202, -0.00491714],\n",
       "       [ 0.00764084, -0.00674438,  0.01120758, ...,  0.00359917,\n",
       "        -0.00379562,  0.01699829]], dtype=float32)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#########################\n",
    "# ## WEIGHT TRANSFER ## #\n",
    "#########################\n",
    "\n",
    "# function for spectral norms\n",
    "def reshape_weight_to_matrix(weight_in):\n",
    "    weight_mat = weight_in\n",
    "    height = weight_mat.shape[0]\n",
    "    return weight_mat.reshape(height, -1)\n",
    "\n",
    "# function to copy linear layer\n",
    "def list_linear_weights(layer_pt):\n",
    "    weight_list = []\n",
    "    # get pt weights\n",
    "    weight_list += [np.transpose(layer_pt.weight.data.numpy())]\n",
    "    if layer_pt.bias is not None:\n",
    "        weight_list += [layer_pt.bias.data.numpy()]\n",
    "    return weight_list\n",
    "\n",
    "# function to copy conv layer\n",
    "def list_conv_weights(layer_pt):\n",
    "    weight_list = []\n",
    "\n",
    "    # add to list\n",
    "    weight_list += [np.transpose(layer_pt.weight.data.numpy(), (2, 3, 1, 0))]\n",
    "    # save bias\n",
    "    if layer_pt.bias is not None:\n",
    "        weight_list += [layer_pt.bias.data.numpy()]\n",
    "\n",
    "    return weight_list\n",
    "\n",
    "# copy batchnorm layer\n",
    "def list_batch_norm(layer_pt):\n",
    "    weight_list = []\n",
    "\n",
    "    # learning scaling and bias for standard batch norm\n",
    "    weight_list += [layer_pt.weight.data.numpy()]\n",
    "    weight_list += [layer_pt.bias.data.numpy()]\n",
    "    # running mean and var records \n",
    "    weight_list += [layer_pt.running_mean.data.numpy()]\n",
    "    weight_list += [layer_pt.running_var.data.numpy()]\n",
    "\n",
    "    return weight_list\n",
    "\n",
    "# copy layer norm layer\n",
    "def list_layer_norm(layer_pt):\n",
    "    weight_list = []\n",
    "\n",
    "    # layer norm scale and shift\n",
    "    weight_list += [layer_pt.weight.data.numpy()]\n",
    "    weight_list += [layer_pt.bias.data.numpy()]\n",
    "\n",
    "    return weight_list\n",
    "\n",
    "# copy multiheaded attention\n",
    "def list_attn_weights(layer_pt, num_heads=12):\n",
    "    weights = []\n",
    "\n",
    "    # input weight\n",
    "    weight_attn = layer_pt.in_proj_weight.data.numpy()\n",
    "    weight_attn = np.transpose(weight_attn, (1, 0))\n",
    "    d_model = weight_attn.shape[0]\n",
    "    weight_attn = np.reshape(weight_attn, (d_model, 3 * num_heads, -1))\n",
    "    \n",
    "    # input weight\n",
    "    bias_attn = layer_pt.in_proj_bias.data.numpy()\n",
    "    bias_attn = np.reshape(bias_attn, (3 * num_heads, -1))\n",
    "\n",
    "    for i in range(3):\n",
    "        weights += [weight_attn[:, (i*num_heads):((i+1)*num_heads)]]\n",
    "        weights += [bias_attn[(i*num_heads):((i+1)*num_heads)]]\n",
    "\n",
    "    # out weight\n",
    "    weight_out = layer_pt.out_proj.weight.data.numpy()\n",
    "    weight_out = np.transpose(weight_out, (1, 0))\n",
    "    weight_out = np.reshape(weight_out, (num_heads, -1, d_model))\n",
    "    weights += [weight_out]\n",
    "    # out bias\n",
    "    weights += [layer_pt.out_proj.bias.data.numpy()]\n",
    "\n",
    "    return weights\n",
    "\n",
    "# mlp weights\n",
    "def list_mlp_weights(layer_pt):\n",
    "    dense_1_weights = list_linear_weights(layer_pt[0])\n",
    "    dense_2_weights = list_linear_weights(layer_pt[2])\n",
    "    return dense_1_weights + dense_2_weights\n",
    "\n",
    "# copy gen block\n",
    "def list_res_block(layer_pt, num_heads=12):\n",
    "    weights = []\n",
    "\n",
    "    # list weights of res block components\n",
    "    weights += list_attn_weights(layer_pt.attn, num_heads=num_heads)\n",
    "    weights += list_layer_norm(layer_pt.ln_1)\n",
    "    weights += list_mlp_weights(layer_pt.mlp)\n",
    "    weights += list_layer_norm(layer_pt.ln_2)\n",
    "    \n",
    "    return weights\n",
    "\n",
    "\n",
    "# vision transformer\n",
    "print('Patch Embeddings for ViT.')\n",
    "# patch conv\n",
    "patch_weights = list_conv_weights(model.visual.conv1)\n",
    "clip_tf.visual.conv1.set_weights(patch_weights)\n",
    "# class embedding\n",
    "class_embedding = np.reshape(model.visual.class_embedding.data.numpy(), (1, 1, -1))\n",
    "clip_tf.visual.class_embedding.assign(class_embedding)\n",
    "# positional embedding\n",
    "positional_embedding = model.visual.positional_embedding.data.numpy()\n",
    "clip_tf.visual.positional_embedding.assign(positional_embedding)\n",
    "# layer norm pre transformer\n",
    "ln_weights = list_layer_norm(model.visual.ln_pre)\n",
    "clip_tf.visual.ln_pre.set_weights(ln_weights)\n",
    "\n",
    "# transformer\n",
    "resblock_weights = []\n",
    "for i in range(len(model.visual.transformer.resblocks)):\n",
    "    print('ViT ResBlock {}'.format(i+1))\n",
    "    # get weights for residual block\n",
    "    resblock_weights += list_res_block(model.visual.transformer.resblocks[i], num_heads=12)\n",
    "clip_tf.visual.transformer.resblocks.set_weights(resblock_weights)\n",
    "\n",
    "# layer norm post transformer\n",
    "ln_weights = list_layer_norm(model.visual.ln_post)\n",
    "clip_tf.visual.ln_post.set_weights(ln_weights)\n",
    "# final proj weight\n",
    "proj_weight = model.visual.proj.data.numpy()\n",
    "clip_tf.visual.proj.assign(proj_weight)\n",
    "\n",
    "\n",
    "# text transformer\n",
    "print('Embeddings for Text Transformer.')\n",
    "# class embedding\n",
    "token_embedding = [model.token_embedding.weight.data.numpy()]\n",
    "clip_tf.token_embedding.set_weights(token_embedding)\n",
    "# positional embedding\n",
    "positional_embedding = model.positional_embedding.data.numpy()\n",
    "clip_tf.positional_embedding.assign(positional_embedding)\n",
    "\n",
    "# transformer\n",
    "resblock_weights = []\n",
    "for i in range(len(model.transformer.resblocks)):\n",
    "    print('Text ResBlock {}'.format(i+1))\n",
    "    # get weights for residual block\n",
    "    resblock_weights += list_res_block(model.transformer.resblocks[i], num_heads=8)\n",
    "clip_tf.transformer.resblocks.set_weights(resblock_weights)\n",
    "\n",
    "# layer norm post transformer\n",
    "ln_weights = list_layer_norm(model.ln_final)\n",
    "clip_tf.ln_final.set_weights(ln_weights)\n",
    "# final proj weight\n",
    "proj_weight = model.text_projection.data.numpy()\n",
    "clip_tf.text_projection.assign(proj_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3136,
     "status": "ok",
     "timestamp": 1640387993035,
     "user": {
      "displayName": "MITCHELL HILL",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "00701271769183393256"
     },
     "user_tz": 300
    },
    "id": "52IS5MqWRrUT",
    "outputId": "dca99e18-8c63-4115-a4fa-3f9520a84df7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error for image encoding:  3.0994415e-06\n",
      "Error for text encoding:  2.3841858e-06\n"
     ]
    }
   ],
   "source": [
    "# test vit component\n",
    "im_out_pt = model.visual(image)\n",
    "im_out_tf = clip_tf.visual(image_tf)\n",
    "\n",
    "print('Error for image encoding: ', np.max(np.abs((im_out_pt.data.numpy() - im_out_tf.numpy()))))\n",
    "\n",
    "# test text component\n",
    "text_out_pt = model.encode_text(text)\n",
    "text_out_tf = clip_tf.encode_text(text_tf)\n",
    "\n",
    "print('Error for text encoding: ', np.max(np.abs((text_out_pt.data.numpy() - text_out_tf.numpy()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2241,
     "status": "ok",
     "timestamp": 1640387995267,
     "user": {
      "displayName": "MITCHELL HILL",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "00701271769183393256"
     },
     "user_tz": 300
    },
    "id": "5d765qT1BPId",
    "outputId": "9fb1e41e-99e1-440c-de2f-f5a2ca0f356c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label probs: [[0.99279356 0.00421069 0.00299576]]\n"
     ]
    }
   ],
   "source": [
    "logits_per_image_tf, _ = clip_tf(image_tf, text_tf)\n",
    "probs = keras.activations.softmax(logits_per_image_tf, axis=1).numpy()\n",
    "\n",
    "print(\"Label probs:\", probs)  # should match pytorch output: [[0.9927937  0.00421068 0.00299572]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "LIRAy-EQrARR"
   },
   "outputs": [],
   "source": [
    "# save tf2 weights\n",
    "clip_tf.save_weights(TF2_FILE)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPj9G9Bp0QcOoik2+fzv3ug",
   "collapsed_sections": [],
   "name": "transfer_clip_tf2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
